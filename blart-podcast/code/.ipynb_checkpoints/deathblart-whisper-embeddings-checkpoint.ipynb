{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db555235",
   "metadata": {},
   "source": [
    "# Generating speech-to-text embeddings using WhisperX\n",
    "\n",
    "**Objective**: To generate accurate transcriptions of audio recordings from several podcasts for later analysis using natural language processing (NLP). \n",
    "\n",
    "**How to get this notebook to work**:\n",
    "1. First, you'll want to follow the setup instructions on the [WhisperX github page](https://github.com/m-bain/whisperX).\n",
    "2. After you have created a conda environment and have all the dependencies installed, you'll also want to make sure that juypter notebook is installed in that environment. To do this, install Jupyter in the environment: `conda install ipykernel -c conda-forge` followed by:  `ipython kernel install --user --name=<envname>`\n",
    "3. Now, you should be able to open jupyter notebook and see the environment name as a kernel environment you can select when you open a new notebook. \n",
    "4. Finally, be sure to update the device and compute type depending on your resources. If you're using a personal computer, it's likely that this model will cause your kernel to die. Cuda is typically specific to GPU computing, so if you're not using a GPU, I recommend picking smaller devices & computing power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01fc4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/safestore/users/lindsey/anaconda3/envs/whisperx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd6a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b3d5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/safestore/users/lindsey/Desktop/deathblart-nlp/data/\"\n",
    "file_list = [\"2015_trimmed\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39381264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015 podcast needs first 30s to be trimmed due to music\n",
    "from pydub import AudioSegment\n",
    "\n",
    "podcast = AudioSegment.from_file(file_path+\"Blart2015.mp3\",format=\"mp3\")\n",
    "\n",
    "# pydub does things in miliseconds\n",
    "thirty_seconds = 30 * 1000\n",
    "trimmed_podcast = podcast[thirty_seconds:]\n",
    "trimmed_podcast.export (\"Blart2015_trimmed.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21bdb136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2015.mp3\n",
      "Detected language: pt (0.69) in first 30s of audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)rocessor_config.json: 100%|█████| 262/262 [00:00<00:00, 1.39MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|█| 1.78k/1.78k [00:00<00:00, 12.2MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|█████| 430/430 [00:00<00:00, 2.89MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████| 85.0/85.0 [00:00<00:00, 619kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████| 1.26G/1.26G [00:22<00:00, 54.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2016.mp3\n",
      "Detected language: en (0.59) in first 30s of audio...\n",
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2017.mp3\n",
      "Detected language: en (0.99) in first 30s of audio...\n",
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2018.mp3\n",
      "Detected language: en (0.53) in first 30s of audio...\n",
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2019.mp3\n",
      "Detected language: en (0.98) in first 30s of audio...\n",
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2020.mp3\n",
      "Detected language: en (0.99) in first 30s of audio...\n",
      "Failed to align segment (\" ♪♪\"): no characters in this segment found in model dictionary, resorting to original...\n",
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2021.mp3\n",
      "Detected language: en (1.00) in first 30s of audio...\n",
      "/safestore/users/lindsey/Desktop/deathblart-nlp/data/Blart2022.mp3\n",
      "Detected language: en (1.00) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "for audio_track in file_list:\n",
    "\n",
    "    audio_file = file_path+\"Blart\"+audio_track+\".mp3\"\n",
    "    batch_size = 16 # reduce if low on GPU mem\n",
    "    \n",
    "    print(audio_file)\n",
    "\n",
    "    #Unaligned transcriptions\n",
    "    audio = whisperx.load_audio(audio_file)\n",
    "    result = model.transcribe(audio, batch_size=batch_size)\n",
    "    \n",
    "    #Aligned transcriptions\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "    \n",
    "    with open(\"Blart\"+audio_track+\".json\",\"w\") as write_file:\n",
    "        json.dump(result[\"segments\"],write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2732cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "whisperx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
